{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18022284",
   "metadata": {},
   "source": [
    "# MPS Annotation Pipeline #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e003e",
   "metadata": {},
   "source": [
    "## Pre-Install ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c72b9",
   "metadata": {},
   "source": [
    "Install [Python](https://www.python.org/downloads/), we recommend version 3.11.X.\n",
    "\n",
    "### Windows ###\n",
    "For Windows, packages like onnx, duckdb, pyarrow, or tiktoken require C/C++ compilation. Therefore you must:\n",
    "1. Download [CMake](https://cmake.org/download/)\n",
    "2. During installation, check the box to \"Add CMake to system PATH for all users\".\n",
    "3. Download [Visual Studio C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n",
    "\n",
    "### MacOS ###\n",
    "MacOS is easier because it comes installed with a C/C++ compiler. \n",
    "1. Run: `xcode-select --install` to install command line utilities like git, make, and clang.\n",
    "2. Download CMake with: `brew install cmake`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e3a0f",
   "metadata": {},
   "source": [
    "## Preparing Your Environment ##\n",
    "1. Activate a virtual environment to avoid dependency hell:\n",
    "    ```\n",
    "    py -3.11 -m venv .venv\n",
    "    .venv\\Scripts\\activate\n",
    "    ```\n",
    "2. Upgrade pip `python.exe -m pip install --upgrade pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968e1d0",
   "metadata": {},
   "source": [
    "## Install LLM Tools and Dependencies ##\n",
    "\n",
    "CurateGPT is another library that uses LLM embeddings to prioritize semantically similar ontology content to text or structured data input. CurateGPT also enables users to suggest new ontology content and programmatically interact with GitHub issue trackers. Find the preprint for CurateGPT [here](https://doi.org/10.48550/arXiv.2411.00046).\n",
    "\n",
    "OntoGPT is based on Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a novel method to extract ontological content from text or structured data authored by [Caufield et al., 2024](https://doi.org/10.1093/bioinformatics/btae104)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b91d0",
   "metadata": {},
   "source": [
    "This may take a few minutes. On install you will note there is a dependency conflict with the packages installed. Selenium 4.34.0 requires urllib3==2.4.0 while OntoGPT requires urllib3<2. Install urllib3 1.26.0 with `pip install \"urllib3<2\"`. This shouldn't cause problems at runtime. If there is a conflict, for using CurateGPT use urllib3==2.4.0 with `pip install \"urllib3==2.4.0\"` and then revert to the previous installation of urllib3 with the aforementioned command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b95d2db",
   "metadata": {},
   "source": [
    "### Learn More ###\n",
    "If you get stuck at anytime, visit the GitHub for either package, or visit the example pages for pre-built Jupyter Notebooks.\n",
    "* [CurateGPT GitHub](https://github.com/monarch-initiative/curategpt/)\n",
    "* CurateGPT examples: [here](https://github.com/monarch-initiative/curategpt/blob/main/notebooks/command-line/)\n",
    "* [OntoGPT GitHub](https://github.com/monarch-initiative/ontogpt/) \n",
    "* OntoGPT examples: [here](https://github.com/monarch-initiative/ontogpt/blob/main/notebooks/)\n",
    "* OntoGPT templates: [here](https://github.com/monarch-initiative/ontogpt/blob/main/src/ontogpt/templates/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d3e30",
   "metadata": {},
   "source": [
    "## Getting Started ##\n",
    "\n",
    "Make sure your OPENAI_API_KEY is in the .env file as OPENAI_API_KEY=examplekey1234. Now let's get to curating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb715dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pipe\n",
    "from dotenv import load_dotenv\n",
    "from pprintpp import pprint as pp\n",
    "from datetime import datetime, date\n",
    "load_dotenv() # load .env\n",
    "\n",
    "# load OpenAI API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# we don't want to use onnx embeddings\n",
    "os.environ[\"CHROMADB_USE_ANN\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbc65d",
   "metadata": {},
   "source": [
    "### Set OpenAI API Key ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!runoak set-apikey -e openai $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ec8fe",
   "metadata": {},
   "source": [
    "### Show OntoGPT Options ###\n",
    "\n",
    "OntoGPT has a --help extension. Unfortunately CurateGPT has no such thing, since it assumes a lot of background knowledge on the part of the user about current ontology editing workflows. The good thing for us is that we are only using it to 1) search the PubMed or Wikipedia APIs and 2) return semantically similar terms to terms missed by OntoGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ontogpt --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd1958",
   "metadata": {},
   "source": [
    "## Search PubMed API or Wikipedia API for a Topic of Interest (TOI) ##\n",
    "\n",
    "In our example, we are interested in Mucopolysaccharidosis (MPS), a rare lysosomal disease, caused by defects in lysosomal enzymes which catabolyze glycosaminoglycans. Learn more [here](https://www.ninds.nih.gov/health-information/disorders/mucopolysaccharidoses).\n",
    "\n",
    "We are going to search PubMed for papers based on the query \"What are mucopolysaccharidosis clinical symptoms?\" based on OpenAI embeddings. CurateGPT depends on <i>indexes</i> for many operations, which are used to provide context for LLM queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813f7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import disable_onnx\n",
    "from curategpt.wrappers.literature.pubmed_wrapper import PubmedWrapper\n",
    "from curategpt.wrappers.clinical.hpoa_wrapper import HPOAWrapper\n",
    "from curategpt import BasicExtractor\n",
    "\n",
    "# Setup\n",
    "extractor = BasicExtractor()\n",
    "pubmed = PubmedWrapper(extractor=extractor)\n",
    "\n",
    "query = \"What are mucopolysaccharidosis clinical symptoms?\"\n",
    "results = pubmed.search(query, expand=False, limit=10)\n",
    "\n",
    "# Save to JSON file\n",
    "output = []\n",
    "for result in results:\n",
    "    if not isinstance(result, tuple) or len(result) < 2:\n",
    "        continue\n",
    "\n",
    "    doc, score = result[0], result[1]\n",
    "    if not isinstance(doc, dict):\n",
    "        continue\n",
    "\n",
    "    output.append({\n",
    "        \"score\": score,\n",
    "        **doc  # includes id, title, abstract, pmcid, etc.\n",
    "    })\n",
    "\n",
    "# Save full JSON results\n",
    "with open(\"pubmed_mps_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Print selected fields\n",
    "for i, item in enumerate(output):\n",
    "    title = item.get(\"title\", \"No title\")\n",
    "    pmid = item.get(\"id\", \"No PMID\")\n",
    "    score = item.get(\"score\", \"No score\")\n",
    "    print(f\"{i+1}. {title}\\n   {pmid}\\n   Score: {score:.6f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b8cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import disable_onnx\n",
    "# from curategpt import BasicExtractor\n",
    "# from curategpt.wrappers.literature.pubmed_wrapper import PubmedWrapper\n",
    "# from curategpt.store.chromadb_adapter import ChromaDBAdapter\n",
    "# from curategpt.wrappers.clinical.hpoa_wrapper import HPOAWrapper\n",
    "\n",
    "# # Step 1: Initialize extractor, store, and PubMed wrapper\n",
    "# extractor = BasicExtractor()\n",
    "# adapter = ChromaDBAdapter(\"chromadb_pubmed\")\n",
    "# pubmed = PubmedWrapper(extractor=extractor)\n",
    "\n",
    "# # Step 2: Ask a question and save retrieved docs into ChromaDB\n",
    "# results = pubmed.search(\"What neurons express VIP?\", limit=10)\n",
    "# for i, (doc, _, _) in enumerate(results):\n",
    "#     title = doc.get(\"title\", \"\")\n",
    "#     abstract = doc.get(\"abstract\", \"\")\n",
    "#     combined = f\"{title}\\n{abstract}\"\n",
    "#     adapter.insert([{\n",
    "#         \"id\": f\"doc_{i}\",\n",
    "#         \"text\": combined\n",
    "#     }], collection=\"pubmed\")\n",
    "\n",
    "# # Step 3: Wrap with HPOAWrapper and extract filtered annotations\n",
    "# wrapper = HPOAWrapper(local_store=adapter, extractor=extractor)\n",
    "# rows = list(wrapper.objects(collection=\"pubmed\"))\n",
    "\n",
    "# # Only keep rows with valid HP: phenotype IDs\n",
    "# filtered_rows = [row for row in rows if str(row.get(\"phenotype_id\", \"\")).startswith(\"HP:\")]\n",
    "\n",
    "# # Step 4: Write the filtered HPOA-formatted output\n",
    "# with open(\"filtered_output.hpoa.tsv\", \"w\") as f:\n",
    "#     for row in filtered_rows:\n",
    "#         f.write(\"\\t\".join([\n",
    "#             row.get(\"disease_id\", \"\"),\n",
    "#             row.get(\"phenotype_id\", \"\"),\n",
    "#             row.get(\"qualifier\", \"\"),\n",
    "#             row.get(\"evidence\", \"\"),\n",
    "#             row.get(\"onset\", \"\"),\n",
    "#             row.get(\"frequency\", \"\"),\n",
    "#             row.get(\"sex\", \"\"),\n",
    "#             row.get(\"modifier\", \"\"),\n",
    "#             row.get(\"aspect\", \"P\"),\n",
    "#             row.get(\"publication\", \"\"),\n",
    "#             row.get(\"description\", \"\"),\n",
    "#         ]) + \"\\n\")\n",
    "\n",
    "# print(f\"âœ… Wrote {len(filtered_rows)} filtered HPOA entries to filtered_output.hpoa.tsv\")\n",
    "\n",
    "\n",
    "# from curategpt import BasicExtractor\n",
    "# from curategpt.wrappers.literature.pubmed_wrapper import PubmedWrapper\n",
    "# from curategpt.wrappers.clinical.hpoa_wrapper import HPOAWrapper\n",
    "# from curategpt.store.chromadb_adapter import ChromaDBAdapter\n",
    "\n",
    "# # Step 1: Search PubMed for MPS symptoms\n",
    "# extractor = BasicExtractor()\n",
    "# pubmed = PubmedWrapper(extractor=extractor)\n",
    "# results = list(pubmed.search(\"Mucopolysaccharidosis clinical symptoms\", limit=15))\n",
    "\n",
    "# # Step 2: Insert PubMed docs into a local ChromaDB\n",
    "# adapter = ChromaDBAdapter(\"hpoa_pubmed_db\")\n",
    "# adapter.client.reset()\n",
    "# objects = []\n",
    "\n",
    "# for i, (doc, score, meta) in enumerate(results):\n",
    "#     text = f\"{doc.get('title', '')}\\n{doc.get('abstract', '')}\".strip()\n",
    "#     if text:\n",
    "#         objects.append({\"id\": f\"doc_{i}\", \"text\": text})\n",
    "\n",
    "# adapter.insert(objects, collection=\"pubmed\")\n",
    "\n",
    "# # Step 3: Extract HPOA-like objects\n",
    "# wrapper = HPOAWrapper(local_store=adapter, extractor=extractor, expand_publications=False)\n",
    "    \n",
    "# # Step 4: Save to TSV\n",
    "# output_file = \"output.mps4.hpoa.tsv\"\n",
    "# seen_rows = set()\n",
    "\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(\"\\t\".join([ # write column headers\n",
    "#     \"disease_id\", \"phenotype_id\", \"qualifier\", \"evidence\", \"onset\", \"frequency\",\n",
    "#     \"sex\", \"modifier\", \"aspect\", \"publication\", \"description\"\n",
    "#     ]) + \"\\n\")\n",
    "#     for row in wrapper.objects(collection=\"pubmed\"):\n",
    "#         # # Optional: filter for only valid HPO phenotype IDs\n",
    "#         # phenotype_id = str(row.get(\"phenotype_id\", \"\"))\n",
    "#         # if not phenotype_id.startswith(\"HP:\"):\n",
    "#         #     continue\n",
    "\n",
    "#         line = \"\\t\".join([\n",
    "#             row.get(\"disease_id\", \"\"),\n",
    "#             row.get(\"phenotype_id\", \"\"),\n",
    "#             row.get(\"qualifier\", \"\"),\n",
    "#             row.get(\"evidence\", \"\"),\n",
    "#             row.get(\"onset\", \"\"),\n",
    "#             row.get(\"frequency\", \"\"),\n",
    "#             row.get(\"sex\", \"\"),\n",
    "#             row.get(\"modifier\", \"\"),\n",
    "#             row.get(\"aspect\", \"P\"),\n",
    "#             row.get(\"publication\", \"\"),\n",
    "#             row.get(\"description\", \"\"),\n",
    "#         ])\n",
    "\n",
    "#         if line not in seen_rows:\n",
    "#             seen_rows.add(line)\n",
    "#             f.write(line + \"\\n\")\n",
    "\n",
    "# print(f\"\\nExtracted {len(seen_rows)} unique HPO terms written to: {output_file}\")\n",
    "\n",
    "from curategpt import BasicExtractor\n",
    "from curategpt.wrappers.literature.pubmed_wrapper import PubmedWrapper\n",
    "from curategpt.wrappers.clinical.hpoa_wrapper import HPOAWrapper\n",
    "from curategpt.store.chromadb_adapter import ChromaDBAdapter\n",
    "\n",
    "# Step 1: Search PubMed for MPS symptoms\n",
    "extractor = BasicExtractor()\n",
    "pubmed = PubmedWrapper(extractor=extractor)\n",
    "results = list(pubmed.search(\"Mucopolysaccharidosis clinical symptoms\", limit=15))\n",
    "\n",
    "# Step 2: Insert PubMed docs into a local ChromaDB\n",
    "adapter = ChromaDBAdapter(\"hpoa_pubmed_db\")\n",
    "adapter.client.reset()\n",
    "objects = []\n",
    "\n",
    "for i, (doc, score, meta) in enumerate(results):\n",
    "    text = f\"{doc.get('title', '')}\\n{doc.get('abstract', '')}\".strip()\n",
    "    pub_id = doc.get(\"id\", \"\")\n",
    "    if text:\n",
    "        objects.append({\n",
    "            \"id\": f\"doc_{i}\",\n",
    "            \"text\": text,\n",
    "            \"publication\": pub_id  # <-- Add PMID here\n",
    "        })\n",
    "\n",
    "adapter.insert(objects, collection=\"pubmed\")\n",
    "\n",
    "# Step 3: Extract HPOA-like objects\n",
    "wrapper = HPOAWrapper(local_store=adapter, extractor=extractor, expand_publications=False)\n",
    "\n",
    "# Step 4: Save to TSV\n",
    "output_file = \"output.mps4.hpoa.tsv\"\n",
    "seen_rows = set()\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\t\".join([  # write column headers\n",
    "        \"disease_id\", \"phenotype_id\", \"qualifier\", \"evidence\", \"onset\", \"frequency\",\n",
    "        \"sex\", \"modifier\", \"aspect\", \"publication\", \"description\"\n",
    "    ]) + \"\\n\")\n",
    "    for row in wrapper.objects(collection=\"pubmed\"):\n",
    "        line = \"\\t\".join([\n",
    "            row.get(\"disease_id\", \"\"),\n",
    "            row.get(\"phenotype_id\", \"\"),\n",
    "            row.get(\"qualifier\", \"\"),\n",
    "            row.get(\"evidence\", \"\"),\n",
    "            row.get(\"onset\", \"\"),\n",
    "            row.get(\"frequency\", \"\"),\n",
    "            row.get(\"sex\", \"\"),\n",
    "            row.get(\"modifier\", \"\"),\n",
    "            row.get(\"aspect\", \"P\"),\n",
    "            row.get(\"publication\", \"\"),  # now correctly filled\n",
    "            row.get(\"description\", \"\"),\n",
    "        ])\n",
    "\n",
    "        if line not in seen_rows:\n",
    "            seen_rows.add(line)\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"\\nExtracted {len(seen_rows)} unique HPO terms written to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9cc39",
   "metadata": {},
   "source": [
    "### Extract Human Phenotype Ontology Terms from MPS Papers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ontogpt -vvv extract -i example1.txt -t templates/human_phenotype.yaml -o output/output.yaml --model-provider openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be94670",
   "metadata": {},
   "source": [
    "### Index HPO For AUTO Prefix Terms ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e829a",
   "metadata": {},
   "source": [
    "Behind the scenes, OAK is used to access a variety of different ontologies and allows them to be indexed. See the oaklib docs for documentation on handles such as sqlite:obo:hp.\n",
    "\n",
    "Let's start by making an index of the Human Phenotype Ontology (HP) and the MONDO Disease Ontology (MONDO):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5fab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curategpt ontology index -m openai: -e openai -c terms_hp sqlite:obo:hp\n",
    "#!curategpt ontology index -m openai: -c terms_hp sqlite:obo:hp\n",
    "#!curategpt ontology index -m openai: -c terms_mondo sqlite:obo:mondo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a8a14",
   "metadata": {},
   "source": [
    "Warning: this currently takes about 2 hrs; if you use OpenAI to embed the terms you will need an OpenAI API key. You can also leave the -m option off and it will use the default chromadb embedding model. gpt-4o is recommended for using CurateGPT.\n",
    "\n",
    "Next, we will determine which terms (prefix AUTO:) could not be matched to HPO by OntoGPT and run a search using OpenAI embeddings for the most similar terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e42c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/output.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "# Extract label and attribute it to a MONDO ID\n",
    "label = data[\"extracted_object\"][\"label\"]\n",
    "#!curategpt search -c terms_mondo label\n",
    "\n",
    "# Extract AUTO terms and find semantically similar phenotypes\n",
    "raw_auto_terms = [item for item in data[\"extracted_object\"][\"phenotypes\"] if item.startswith(\"AUTO:\")]\n",
    "auto_terms = [item.replace(\"AUTO:\", \"\").replace(\"%20\", \" \") for item in raw_auto_terms]\n",
    "\n",
    "print(auto_terms)\n",
    "\n",
    "#!curategpt search -c terms_hp auto_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755501f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (save):\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        filename = f\"article_urls_{timestamp}.json\"\n",
    "        output_path = os.path.join(\"requests//json\", filename)\n",
    "\n",
    "        # save json\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curategpt ask -c phenopackets_384 \"what genes are associated with renal phenotypes?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f50ae6d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
